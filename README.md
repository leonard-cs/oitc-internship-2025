# AnythingLLM + Ollama Vector Database Query

This project demonstrates how to query SQL data stored in a vector database using natural language, powered by AnythingLLM and Ollama running inside Docker containers. Users can ask questions in plain English, and the system returns direct answers generated by the language model.

## Project Overview
Goal: Enable natural language querying of SQL data indexed in a vector database.
Technologies:
- AnythingLLM (for LLM + vector retrieval)
- Ollama (for hosting language models in Docker)
- Python (API client to interact with AnythingLLM server)
- Vector database (storing embeddings of your SQL data)
How it works:
- SQL data rows are formatted as JSON and uploaded to the vector database.
- When a user inputs a natural language query, the system retrieves relevant data chunks from the vector DB.
- The language model generates an answer directly (without exposing raw SQL).
- The answer and relevant source chunks are returned to the user.

## Project Structure
```bash
llm-server/
├─db_uploader
│  ├─ extract_mssql.py
│  └─ upload_to_anythingllm.py
├── anythingllm_query.py
└── run.py
```

## Setup Instructions

## APIs
- `{LLM_API_URL}/api/v1/document/upload`
- `{LLM_API_URL}/api/v1/workspace/{workspace_slug}/chat`

## References
- [AnythingLLM Docker](https://docs.anythingllm.com/installation-docker/local-docker)
- [AnythingLLM API](https://docs.anythingllm.com/features/api)