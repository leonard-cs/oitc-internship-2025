# === LLM Configuration ===
LLM_PROVIDER=ollama                          # Options: ollama, openai, etc.
OLLAMA_BASE_PATH=http://ollama:11434        # Internal Docker address for Ollama
OLLAMA_MODEL_PREF=phi3:latest               # Model name/tag (e.g., llama3:instruct)
OLLAMA_MODEL_TOKEN_LIMIT=4096               # Token limit for the chosen model
OLLAMA_PERFORMANCE_MODE=maximum                # Options: base, maximum
OLLAMA_KEEP_ALIVE_TIMEOUT=300               # Seconds to keep model active (idle timeout)

# === Embedding & Vector Database ===
EMBEDDING_ENGINE=native                     # Options: native, openai, etc.
VECTOR_DB=lancedb                           # Options: lancedb, chroma, etc.

# === Storage ===
STORAGE_DIR=/app/server/storage             # Path inside container for storing files and embeddings
